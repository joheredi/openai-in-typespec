using TypeSpec.Http;
using OpenAPI;

@service({
  title: "OpenAI API",

  // todo: this is not yet supported.
  description: "APIs for sampling from the fine-tuning language models",

  version: "2.0.0",
})
namespace OpenAI;

model CreateChatCompletionRequest {
  /**
   * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)
   * table for details on which models work with the Chat API.
   */
  @extension("x-oaiTypeLabel", "string")
  `model`:
    | string
    | "gpt4"
    | "gpt-4-0314"
    | "gpt-4-0613"
    | "gpt-4-32k"
    | "gpt-4-32k-0314"
    | "gpt-4-32k-0613"
    | "gpt-3.5-turbo"
    | "gpt-3.5-turbo-16k"
    | "gpt-3.5-turbo-0301"
    | "gpt-3.5-turbo-0613"
    | "gpt-3.5-turbo-16k-0613";

  /**
   * A list of messages comprising the conversation so far.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
   */
  @minItems(1)
  messages: ChatCompletionRequestMessage[];

  /**
   * A list of functions the model may generate JSON inputs for.
   */
  @minItems(1)
  @maxItems(128)
  functions?: ChatCompletionFunctions[];

  /**
   * Controls how the model responds to function calls. "none" means the model does not call a
   * function, and responds to the end-user. "auto" means the model can pick between an end-user or
   * calling a function.  Specifying a particular function via `{"name":\ "my_function"}` forces the
   * model to call that function. "none" is the default when no functions are present. "auto" is the
   * default if functions are present.
   */
  function_call?: "none" | "auto" | ChatCompletionFunctionCallOption;

  // todo: needs default value
  // https://github.com/microsoft/typespec/issues/2354
  /**
   * *completions_temperature_description
   */
  temperature: Temperature | null;

  // todo: needs default value
  /**
   * *completions_top_p_description
   */
  top_p?: TopP | null;

  // todo: needs default value
  /**
   * How many chat completion choices to generate for each input message.
   */
  n?: N | null;

  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
   * as they become available, with the stream terminated by a `data: [DONE]` message.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
   */
  stream?: boolean | null = true;

  // todo: needs a null default
  // todo: needs to be oneOf
  // https://github.com/microsoft/typespec/issues/2355
  /**
   * Up to 4 sequences where the API will stop generating further tokens.
   */
  stop?: string | StopSequences | null;

  // todo: default is 'inf' in the OAS but that seems bogus
  /**
   * The maximum number of [tokens](/tokenizer) to generate in the chat completion.
   *
   * The total length of input tokens and generated tokens is limited by the model's context length.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
   * for counting tokens.
   */
  max_tokens?: safeint;

  // needs default
  // https://github.com/microsoft/typespec/issues/1646
  /**
   * *completions_presence_penalty_description
   */
  presence_penalty?: Penalty | null;

  // needs default
  /**
   * *completions_frequency_penalty_description
   */
  frequency_penalty?: Penalty | null;

  // needs default of null
  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
   * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
   * generated by the model prior to sampling. The exact effect will vary per model, but values
   * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
   * should result in a ban or exclusive selection of the relevant token.
   */
  @extension("x-oaiTypeLabel", "map")
  logit_bias?: Record<safeint> | null;

  user?: User;
}

// this is using yaml refs instead of a def in the openapi, not sure if that's required?
/**
 * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
 * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
 */
scalar User extends string;

// TODO: these types are required because we cannot put constraints on a nullable field :/
// https://github.com/microsoft/typespec/issues/2353
@minValue(0)
@maxValue(1)
scalar Temperature extends float64;

@minValue(0)
@maxValue(1)
scalar TopP extends float64;

@minValue(1)
@maxValue(128)
scalar N extends safeint;

@minItems(1)
@maxItems(4)
model StopSequences is string[];

@minValue(-2)
@maxValue(2)
scalar Penalty extends float64;

model ChatCompletionFunctionCallOption {
  /**
   * The name of the function to call.
   */
  name: string;
}

model ChatCompletionFunctions {
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
   * dashes, with a maximum length of 64.
   */
  name: string;

  /**
   * A description of what the function does, used by the model to choose when and how to call the
   * function.
   */
  description?: string;

  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the
   * [guide](/docs/guides/gpt/function-calling) for examples, and the
   * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation
   * about the format.\n\nTo describe a function that accepts no parameters, provide the value
   * `{\"type\": \"object\", \"properties\": {}}`.
   */
  parameters: ChatCompletionFunctionParameters;
}

model ChatCompletionFunctionParameters is Record<unknown>;

model ChatCompletionRequestMessage {
  /**
   * The role of the messages author. One of `system`, `user`, `assistant`, or `function`.
   */
  role: "system" | "user" | "assistant" | "function";

  /**
   * The contents of the message. `content` is required for all messages, and may be null for
   * assistant messages with function calls.
   */
  content: string | null;

  // TODO: the constraints are not specified in the API
  /**
   * The name of the author of this message. `name` is required if role is `function`, and it
   * should be the name of the function whose response is in the `content`. May contain a-z,
   * A-Z, 0-9, and underscores, with a maximum length of 64 characters.
   */
  name?: string;

  /**
   * The name and arguments of a function that should be called, as generated by the model.
   */
  function_call?: {
    /**
     * The name of the function to call.
     */
    name: string;

    /**
     * The arguments to call the function with, as generated by the model in JSON format. Note that
     * the model does not always generate valid JSON, and may hallucinate parameters not defined by
     * your function schema. Validate the arguments in your code before calling your function.
     */
    arguments: string;
  };
}

/**
 * Represents a chat completion response returned by model, based on the provided input.
 */
// TODO: Fill in example here.
@extension(
  "x-oaiMeta",
  {
    name: "The chat completion object",
    group: "chat",
    example: "",
  }
)
model CreateChatCompletionResponse {
  /**
   * A unique identifier for the chat completion.
   */
  id: string;

  /**
   * The object type, which is always `chat.completion`.
   */
  object: string;

  // todo: unsure if this is correct since I think unix timestamps have ms granularity?
  /**
   * The Unix timestamp (in seconds) of when the chat completion was created.
   */
  @encode("unixTimestamp", int32)
  created: utcDateTime;

  /**
   * The model used for the chat completion.
   */
  `model`: string;

  /**
   * A list of chat completion choices. Can be more than one if `n` is greater than 1.
   */
  choices: {
    /**
     * The index of the choice in the list of choices.
     */
    index: safeint;

    message: ChatCompletionResponseMessage;

    /**
     * The reason the model stopped generating tokens. This will be `stop` if the model hit a
     * natural stop point or a provided stop sequence, `length` if the maximum number of tokens
     * specified in the request was reached, or `function_call` if the model called a function.
     */
    finish_reason: "stop" | "length" | "function_call";
  };

  usage?: string;
}

model ChatCompletionResponseMessage {
  /**
   * The role of the author of this message.
   */
  role: "system" | "user" | "assistant" | "function";

  /**
   * The contents of the message.
   */
  content: string | null;

  /**
   * The name and arguments of a function that should be called, as generated by the model.
   */
  function_call?: {
    /**
     * The name of the function to call.
     */
    name: string;

    /**
     * The arguments to call the function with, as generated by the model in JSON format. Note that
     * the model does not always generate valid JSON, and may hallucinate parameters not defined by
     * your function schema. Validate the arguments in your code before calling your function.
     */
    arguments: string;
  };
}

@route("/chat")
interface Chat {
  @post createChatCompletion(
    @body body: CreateChatCompletionRequest,
  ): CreateChatCompletionResponse;
}
