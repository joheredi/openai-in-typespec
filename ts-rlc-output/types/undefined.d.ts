import { Client } from '@azure-rest/core-client';
import { ClientOptions } from '@azure-rest/core-client';
import { ErrorModel } from '@azure-rest/core-client';
import { HttpResponse } from '@azure-rest/core-client';
import { RequestParameters } from '@azure-rest/core-client';
import { StreamableMethod } from '@azure-rest/core-client';

export declare interface ChatCompletionFunctionCallOption {
    /** The name of the function to call. */
    name: string;
}

export declare interface ChatCompletionFunctions {
    /**
     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
     * dashes, with a maximum length of 64.
     */
    name: string;
    /**
     * A description of what the function does, used by the model to choose when and how to call the
     * function.
     */
    description?: string;
    /**
     * The parameters the functions accepts, described as a JSON Schema object. See the
     * [guide](/docs/guides/gpt/function-calling) for examples, and the
     * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation
     * about the format.\n\nTo describe a function that accepts no parameters, provide the value
     * `{\"type\": \"object\", \"properties\": {}}`.
     */
    parameters: Record<string, unknown>;
}

export declare interface ChatCompletionRequestMessage {
    /** The role of the messages author. One of `system`, `user`, `assistant`, or `function`. */
    role: "system" | "user" | "assistant" | "function";
    /**
     * The contents of the message. `content` is required for all messages, and may be null for
     * assistant messages with function calls.
     */
    content: string | null;
    /**
     * The name of the author of this message. `name` is required if role is `function`, and it
     * should be the name of the function whose response is in the `content`. May contain a-z,
     * A-Z, 0-9, and underscores, with a maximum length of 64 characters.
     */
    name?: string;
    /** The name and arguments of a function that should be called, as generated by the model. */
    function_call?: object;
}

export declare interface ChatCompletionResponseMessageOutput {
    /** The role of the author of this message. */
    role: "system" | "user" | "assistant" | "function";
    /** The contents of the message. */
    content: string | null;
    /** The name and arguments of a function that should be called, as generated by the model. */
    function_call?: Record<string, any>;
}

export declare interface CompletionsCreate {
    post(options: CompletionsCreateParameters): StreamableMethod<CompletionsCreate200Response | CompletionsCreateDefaultResponse>;
}

export declare interface CompletionsCreate {
    post(options: CompletionsCreateParameters): StreamableMethod<CompletionsCreate200Response | CompletionsCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface CompletionsCreate200Response extends HttpResponse {
    status: "200";
    body: CreateChatCompletionResponseOutput;
}

/** The request has succeeded. */
export declare interface CompletionsCreate200Response extends HttpResponse {
    status: "200";
    body: CreateCompletionResponseOutput;
}

export declare interface CompletionsCreateBodyParam {
    body: CreateChatCompletionRequest;
}

export declare interface CompletionsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface CompletionsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type CompletionsCreateParameters = CompletionsCreateBodyParam & RequestParameters;

/** Usage statistics for the completion request. */
export declare interface CompletionUsageOutput {
    /** Number of tokens in the prompt. */
    prompt_tokens: number;
    /** Number of tokens in the generated completion */
    completion_tokens: number;
    /** Total number of tokens used in the request (prompt + completion). */
    total_tokens: number;
}

export declare interface CreateChatCompletionRequest {
    /**
     * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)
     * table for details on which models work with the Chat API.
     */
    model: string | "gpt4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0301" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-16k-0613";
    /**
     * A list of messages comprising the conversation so far.
     * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
     */
    messages: Array<ChatCompletionRequestMessage>;
    /** A list of functions the model may generate JSON inputs for. */
    functions?: Array<ChatCompletionFunctions>;
    /**
     * Controls how the model responds to function calls. "none" means the model does not call a
     * function, and responds to the end-user. "auto" means the model can pick between an end-user or
     * calling a function.  Specifying a particular function via `{"name":\ "my_function"}` forces the
     * model to call that function. "none" is the default when no functions are present. "auto" is the
     * default if functions are present.
     */
    function_call?: "none" | "auto" | ChatCompletionFunctionCallOption;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
     * more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers
     * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
     * the top 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     */
    top_p?: number | null;
    /**
     * How many completions to generate for each prompt.
     * **Note:** Because this parameter generates many completions, it can quickly consume your token
     * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
     */
    n?: number | null;
    /**
     * The maximum number of [tokens](/tokenizer) to generate in the completion.
     *
     * The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
     * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
     * for counting tokens.
     */
    max_tokens?: number | null;
    /** Up to 4 sequences where the API will stop generating further tokens. */
    stop?: string | string[] | null;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
     * in the text so far, increasing the model's likelihood to talk about new topics.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
     */
    presence_penalty?: number | null;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
     * frequency in the text so far, decreasing the model's likelihood to repeat the same line
     * verbatim.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
     */
    frequency_penalty?: number | null;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
     * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
     * generated by the model prior to sampling. The exact effect will vary per model, but values
     * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     */
    logit_bias?: Record<string, number> | null;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
     * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     */
    user?: string;
    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
     * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
     * as they become available, with the stream terminated by a `data: [DONE]` message.
     * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
     */
    stream?: boolean | null;
}

/** Represents a chat completion response returned by model, based on the provided input. */
export declare interface CreateChatCompletionResponseOutput {
    /** A unique identifier for the chat completion. */
    id: string;
    /** The object type, which is always `chat.completion`. */
    object: string;
    /** The Unix timestamp (in seconds) of when the chat completion was created. */
    created: number;
    /** The model used for the chat completion. */
    model: string;
    /** A list of chat completion choices. Can be more than one if `n` is greater than 1. */
    choices: Record<string, any>[];
    usage?: CompletionUsageOutput;
}

/**
 * Initialize a new instance of `OpenAIClient`
 * @param endpoint - The parameter endpoint
 * @param options - the parameter for all optional parameters
 */
declare function createClient(endpoint: string, options?: ClientOptions): OpenAIClient;
export default createClient;

export declare interface CreateCompletionRequest {
    /**
     * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
     * see all of your available models, or see our [Model overview](/docs/models/overview) for
     * descriptions of them.
     */
    model: string | "babbage-002" | "davinci-002" | "text-davinci-003" | "text-davinci-002" | "text-davinci-001" | "code-davinci-002" | "text-curie-001" | "text-babbage-001" | "text-ada-001";
    /**
     * The prompt(s) to generate completions for, encoded as a string, array of strings, array of
     * tokens, or array of token arrays.
     *
     * Note that <|endoftext|> is the document separator that the model sees during training, so if a
     * prompt is not specified the model will generate as if from the beginning of a new document.
     */
    prompt: string | string[] | number[] | number[][] | null;
    /** The suffix that comes after a completion of inserted text. */
    suffix?: string | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
     * more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers
     * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
     * the top 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     */
    top_p?: number | null;
    /**
     * How many completions to generate for each prompt.
     * **Note:** Because this parameter generates many completions, it can quickly consume your token
     * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
     */
    n?: number | null;
    /**
     * The maximum number of [tokens](/tokenizer) to generate in the completion.
     *
     * The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
     * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
     * for counting tokens.
     */
    max_tokens?: number | null;
    /** Up to 4 sequences where the API will stop generating further tokens. */
    stop?: string | string[] | null;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
     * in the text so far, increasing the model's likelihood to talk about new topics.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
     */
    presence_penalty?: number | null;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
     * frequency in the text so far, decreasing the model's likelihood to repeat the same line
     * verbatim.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
     */
    frequency_penalty?: number | null;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
     * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
     * generated by the model prior to sampling. The exact effect will vary per model, but values
     * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     */
    logit_bias?: Record<string, number> | null;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
     * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     */
    user?: string;
    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
     * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
     * as they become available, with the stream terminated by a `data: [DONE]` message.
     * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
     */
    stream?: boolean | null;
    /**
     * Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.
     * For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The
     * API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`
     * elements in the response.
     *
     * The maximum value for `logprobs` is 5.
     */
    logprobs?: number | null;
    /** Echo back the prompt in addition to the completion */
    echo?: boolean | null;
    /**
     * Generates `best_of` completions server-side and returns the "best" (the one with the highest
     * log probability per token). Results cannot be streamed.
     *
     * When used with `n`, `best_of` controls the number of candidate completions and `n` specifies
     * how many to return – `best_of` must be greater than `n`.
     *
     * **Note:** Because this parameter generates many completions, it can quickly consume your token
     * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
     */
    best_of: number | null;
}

/**
 * Represents a completion response from the API. Note: both the streamed and non-streamed response
 * objects share the same shape (unlike the chat endpoint).
 */
export declare interface CreateCompletionResponseOutput {
    /** A unique identifier for the completion. */
    id: string;
    /** The object type, which is always `text_completion`. */
    object: string;
    /** The Unix timestamp (in seconds) of when the completion was created. */
    created: number;
    /** The model used for the completion. */
    model: string;
    /** The list of completion choices the model generated for the input. */
    choices: Record<string, any>[];
    usage?: CompletionUsageOutput;
}

export declare interface CreateEditRequest {
    /**
     * ID of the model to use. You can use the `text-davinci-edit-001` or `code-davinci-edit-001`
     * model with this endpoint.
     */
    model: string | "text-davinci-edit-001" | "code-davinci-edit-001";
    /** The input text to use as a starting point for the edit. */
    input?: string | null;
    /** The instruction that tells the model how to edit the prompt. */
    instruction: string;
    /** How many edits to generate for the input and instruction. */
    n?: number | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
     * more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers
     * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
     * the top 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     */
    top_p?: number | null;
}

export declare interface CreateEditResponseOutput {
    /** The object type, which is always `edit`. */
    object: "edit";
    /** The Unix timestamp (in seconds) of when the edit was created. */
    created: number;
    /** description: A list of edit choices. Can be more than one if `n` is greater than 1. */
    choices: Record<string, any>[];
    usage: CompletionUsageOutput;
}

export declare interface CreateEmbeddingRequest {
    /** ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them. */
    model: string | "text-embedding-ada-002";
    /**
     * Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a
     * single request, pass an array of strings or array of token arrays. Each input must not exceed
     * the max input tokens for the model (8191 tokens for `text-embedding-ada-002`).
     * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
     * for counting tokens.
     */
    input: string | string[] | number[] | number[][];
    user?: string;
}

export declare interface CreateEmbeddingResponseOutput {
    /** The object type, which is always "embedding". */
    object: "embedding";
    /** The name of the model used to generate the embedding. */
    model: string;
    /** The list of embeddings generated by the model. */
    data: Array<EmbeddingOutput>;
    /** The usage information for the request. */
    usage: Record<string, any>;
}

export declare interface CreateFileRequest {
    /**
     * Name of the [JSON Lines](https://jsonlines.readthedocs.io/en/latest/) file to be uploaded.
     *
     * If the `purpose` is set to "fine-tune", the file will be used for fine-tuning.
     */
    file: string;
    /**
     * The intended purpose of the uploaded documents. Use "fine-tune" for
     * [fine-tuning](/docs/api-reference/fine-tuning). This allows us to validate the format of the
     * uploaded file.
     */
    purpose: string;
}

export declare interface CreateFineTuneRequest {
    /**
     * The ID of an uploaded file that contains training data.
     *
     * See [upload file](/docs/api-reference/files/upload) for how to upload a file.
     *
     * Your dataset must be formatted as a JSONL file, where each training example is a JSON object
     * with the keys "prompt" and "completion". Additionally, you must upload your file with the
     * purpose `fine-tune`.
     *
     * See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more
     * details.
     */
    training_file: string;
    /**
     * The ID of an uploaded file that contains validation data.
     *
     * If you provide this file, the data is used to generate validation metrics periodically during
     * fine-tuning. These metrics can be viewed in the
     * [fine-tuning results file](/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).
     * Your train and validation data should be mutually exclusive.
     *
     * Your dataset must be formatted as a JSONL file, where each validation example is a JSON object
     * with the keys "prompt" and "completion". Additionally, you must upload your file with the
     * purpose `fine-tune`.
     *
     * See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more
     * details.
     */
    validation_file?: string | null;
    /**
     * The name of the base model to fine-tune. You can select one of "ada", "babbage", "curie",
     * "davinci", or a fine-tuned model created after 2022-04-21 and before 2023-08-22. To learn more
     * about these models, see the [Models](/docs/models) documentation.
     */
    model?: string | "ada" | "babbage" | "curie" | "davinci" | null;
    /**
     * The number of epochs to train the model for. An epoch refers to one full cycle through the
     * training dataset.
     */
    n_epochs?: number | null;
    /**
     * The batch size to use for training. The batch size is the number of training examples used to
     * train a single forward and backward pass.
     *
     * By default, the batch size will be dynamically configured to be ~0.2% of the number of examples
     * in the training set, capped at 256 - in general, we've found that larger batch sizes tend to
     * work better for larger datasets.
     */
    batch_size?: number | null;
    /**
     * The learning rate multiplier to use for training. The fine-tuning learning rate is the original
     * learning rate used for pretraining multiplied by this value.
     *
     * By default, the learning rate multiplier is the 0.05, 0.1, or 0.2 depending on final
     * `batch_size` (larger learning rates tend to perform better with larger batch sizes). We
     * recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best
     * results.
     */
    learning_rate_multiplier?: number | null;
    /**
     * The weight to use for loss on the prompt tokens. This controls how much the model tries to
     * learn to generate the prompt (as compared to the completion which always has a weight of 1.0),
     * and can add a stabilizing effect to training when completions are short.
     *
     * If prompts are extremely long (relative to completions), it may make sense to reduce this
     * weight so as to avoid over-prioritizing learning the prompt.
     */
    prompt_loss_rate?: number | null;
    /**
     * If set, we calculate classification-specific metrics such as accuracy and F-1 score using the
     * validation set at the end of every epoch. These metrics can be viewed in the
     * [results file](/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).
     *
     * In order to compute classification metrics, you must provide a `validation_file`. Additionally,
     * you must specify `classification_n_classes` for multiclass classification or
     * `classification_positive_class` for binary classification.
     */
    compute_classification_metrics?: boolean | null;
    /**
     * The number of classes in a classification task.
     *
     * This parameter is required for multiclass classification.
     */
    classification_n_classes?: number | null;
    /**
     * The positive class in binary classification.
     *
     * This parameter is needed to generate precision, recall, and F1 metrics when doing binary
     * classification.
     */
    classification_positive_class?: string | null;
    /**
     * If this is provided, we calculate F-beta scores at the specified beta values. The F-beta score
     * is a generalization of F-1 score. This is only used for binary classification.
     *
     * With a beta of 1 (i.e. the F-1 score), precision and recall are given the same weight. A larger
     * beta score puts more weight on recall and less on precision. A smaller beta score puts more
     * weight on precision and less on recall.
     */
    classification_betas?: number[] | null;
    /**
     * A string of up to 40 characters that will be added to your fine-tuned model name.
     *
     * For example, a `suffix` of "custom-model-name" would produce a model name like
     * `ada:ft-your-org:custom-model-name-2022-02-15-04-21-04`.
     */
    suffix: string | null;
}

export declare interface CreateFineTuningJobRequest {
    /**
     * The ID of an uploaded file that contains training data.
     *
     * See [upload file](/docs/api-reference/files/upload) for how to upload a file.
     *
     * Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with
     * the purpose `fine-tune`.
     *
     * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
     */
    training_file: string;
    /**
     * The ID of an uploaded file that contains validation data.
     *
     * If you provide this file, the data is used to generate validation metrics periodically during
     * fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should
     * not be present in both train and validation files.
     *
     * Your dataset must be formatted as a JSONL file. You must upload your file with the purpose
     * `fine-tune`.
     *
     * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
     */
    validation_file?: string | null;
    /**
     * The name of the model to fine-tune. You can select one of the
     * [supported models](/docs/guides/fine-tuning/what-models-can-be-fine-tuned).
     */
    model: string | "babbage-002" | "davinci-002" | "gpt-3.5-turbo";
    /** The hyperparameters used for the fine-tuning job. */
    hyperparameters?: object;
    /**
     * A string of up to 40 characters that will be added to your fine-tuned model name.
     *
     * For example, a `suffix` of "custom-model-name" would produce a model name like
     * `ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel`.
     */
    suffix?: string | null;
}

export declare interface CreateImageEditRequest {
    /** A text description of the desired image(s). The maximum length is 1000 characters. */
    prompt: string;
    /**
     * The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not
     * provided, image must have transparency, which will be used as the mask.
     */
    image: string;
    /**
     * An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where
     * `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions
     * as `image`.
     */
    mask?: string;
    /** The number of images to generate. Must be between 1 and 10. */
    n?: number | null;
    /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */
    size?: "256x256" | "512x512" | "1024x1024" | null;
    /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */
    response_format?: "url" | "b64_json" | null;
    user?: string;
}

export declare interface CreateImageRequest {
    /** A text description of the desired image(s). The maximum length is 1000 characters. */
    prompt: string;
    /** The number of images to generate. Must be between 1 and 10. */
    n?: number | null;
    /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */
    size?: "256x256" | "512x512" | "1024x1024" | null;
    /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */
    response_format?: "url" | "b64_json" | null;
    user?: string;
}

export declare interface CreateImageVariationRequest {
    /**
     * The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB,
     * and square.
     */
    image: string;
    /** The number of images to generate. Must be between 1 and 10. */
    n?: number | null;
    /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */
    size?: "256x256" | "512x512" | "1024x1024" | null;
    /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */
    response_format?: "url" | "b64_json" | null;
    user?: string;
}

export declare interface CreateModerationRequest {
    /** The input text to classify */
    input: string | string[];
    /**
     * Two content moderations models are available: `text-moderation-stable` and
     * `text-moderation-latest`. The default is `text-moderation-latest` which will be automatically
     * upgraded over time. This ensures you are always using our most accurate model. If you use
     * `text-moderation-stable`, we will provide advanced notice before updating the model. Accuracy
     * of `text-moderation-stable` may be slightly lower than for `text-moderation-latest`.
     */
    model?: string | "text-moderation-latest" | "text-moderation-stable";
}

export declare interface CreateModerationResponseOutput {
    /** The unique identifier for the moderation request. */
    id: string;
    /** The model used to generate the moderation results. */
    model: string;
    /** A list of moderation objects. */
    results: Record<string, any>[];
}

export declare interface CreateTranscriptionRequest {
    /**
     * The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4,
     * mpeg, mpga, m4a, ogg, wav, or webm.
     */
    file: string;
    /** ID of the model to use. Only `whisper-1` is currently available. */
    model: string | "whisper-1";
    /**
     * An optional text to guide the model's style or continue a previous audio segment. The
     * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
     */
    prompt?: string;
    /**
     * The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
     * vtt.
     */
    response_format?: "json" | "text" | "srt" | "verbose_json" | "vtt";
    /**
     * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
     * random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
     * the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
     * automatically increase the temperature until certain thresholds are hit.
     */
    temperature?: number;
    /**
     * The language of the input audio. Supplying the input language in
     * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy
     * and latency.
     */
    language?: string;
}

export declare interface CreateTranscriptionResponseOutput {
    text: string;
}

export declare interface CreateTranslationRequest {
    /**
     * The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4,
     * mpeg, mpga, m4a, ogg, wav, or webm.
     */
    file: string;
    /** ID of the model to use. Only `whisper-1` is currently available. */
    model: string | "whisper-1";
    /**
     * An optional text to guide the model's style or continue a previous audio segment. The
     * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
     */
    prompt?: string;
    /**
     * The format of the transcript output, in one of these options: json, text, srt, verbose_json, or
     * vtt.
     */
    response_format?: "json" | "text" | "srt" | "verbose_json" | "vtt";
    /**
     * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more
     * random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,
     * the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to
     * automatically increase the temperature until certain thresholds are hit.
     */
    temperature?: number;
}

export declare interface CreateTranslationResponseOutput {
    text: string;
}

export declare interface DeleteFileResponseOutput {
    id: string;
    object: string;
    deleted: boolean;
}

export declare interface DeleteModelResponseOutput {
    id: string;
    object: string;
    deleted: boolean;
}

export declare interface EditsCreate {
    post(options: EditsCreateParameters): StreamableMethod<EditsCreate200Response | EditsCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface EditsCreate200Response extends HttpResponse {
    status: "200";
    body: CreateEditResponseOutput;
}

export declare interface EditsCreateBodyParam {
    body: CreateEditRequest;
}

export declare interface EditsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type EditsCreateParameters = EditsCreateBodyParam & RequestParameters;

/** Represents an embedding vector returned by embedding endpoint. */
export declare interface EmbeddingOutput {
    /** The index of the embedding in the list of embeddings. */
    index: number;
    /** The object type, which is always "embedding". */
    object: "embedding";
    /**
     * The embedding vector, which is a list of floats. The length of vector depends on the model as\
     * listed in the [embedding guide](/docs/guides/embeddings).
     */
    embedding: number[];
}

export declare interface EmbeddingsCreate {
    post(options: EmbeddingsCreateParameters): StreamableMethod<EmbeddingsCreate200Response | EmbeddingsCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface EmbeddingsCreate200Response extends HttpResponse {
    status: "200";
    body: CreateEmbeddingResponseOutput;
}

export declare interface EmbeddingsCreateBodyParam {
    body: CreateEmbeddingRequest;
}

export declare interface EmbeddingsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type EmbeddingsCreateParameters = EmbeddingsCreateBodyParam & RequestParameters;

export declare interface ErrorResponseOutput {
    error: ErrorModel;
}

/** The request has succeeded. */
export declare interface FilesCreate200Response extends HttpResponse {
    status: "200";
    body: OpenAIFileOutput;
}

export declare interface FilesCreateBodyParam {
    body: FilesCreateFormBody;
}

export declare interface FilesCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface FilesCreateFormBody {
    file: string;
    purpose: string;
}

export declare interface FilesCreateMediaTypesParam {
    contentType: "multipart/form-data";
}

export declare type FilesCreateParameters = FilesCreateMediaTypesParam & FilesCreateBodyParam & RequestParameters;

/** The request has succeeded. */
export declare interface FilesDeleteOperation200Response extends HttpResponse {
    status: "200";
    body: DeleteFileResponseOutput;
}

export declare interface FilesDeleteOperationDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FilesDeleteParameters = RequestParameters;

export declare interface FilesDownload {
    get(options?: FilesDownloadParameters): StreamableMethod<FilesDownload200Response | FilesDownloadDefaultResponse>;
}

/** The request has succeeded. */
export declare interface FilesDownload200Response extends HttpResponse {
    status: "200";
    body: string;
}

export declare interface FilesDownloadDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FilesDownloadParameters = RequestParameters;

export declare interface FilesList {
    get(options?: FilesListParameters): StreamableMethod<FilesList200Response | FilesListDefaultResponse>;
    post(options: FilesCreateParameters): StreamableMethod<FilesCreate200Response | FilesCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface FilesList200Response extends HttpResponse {
    status: "200";
    body: ListFilesResponseOutput;
}

export declare interface FilesListDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FilesListParameters = RequestParameters;

export declare interface FilesRetrieve {
    post(options?: FilesRetrieveParameters): StreamableMethod<FilesRetrieve200Response | FilesRetrieveDefaultResponse>;
    delete(options?: FilesDeleteParameters): StreamableMethod<FilesDeleteOperation200Response | FilesDeleteOperationDefaultResponse>;
}

/** The request has succeeded. */
export declare interface FilesRetrieve200Response extends HttpResponse {
    status: "200";
    body: OpenAIFileOutput;
}

export declare interface FilesRetrieveDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FilesRetrieveParameters = RequestParameters;

export declare interface FineTuneEventOutput {
    object: string;
    created_at: number;
    level: string;
    message: string;
}

/** The `FineTune` object represents a legacy fine-tune job that has been created through the API. */
export declare interface FineTuneOutput {
    /** The object identifier, which can be referenced in the API endpoints. */
    id: string;
    /** The object type, which is always "fine-tune". */
    object: "fine-tune";
    /** The Unix timestamp (in seconds) for when the fine-tuning job was created. */
    created_at: number;
    /** The Unix timestamp (in seconds) for when the fine-tuning job was last updated. */
    updated_at: number;
    /** The base model that is being fine-tuned. */
    model: string;
    /** The name of the fine-tuned model that is being created. */
    fine_tuned_model: string | null;
    /** The organization that owns the fine-tuning job. */
    organization_id: string;
    /**
     * The current status of the fine-tuning job, which can be either `created`, `running`,
     * `succeeded`, `failed`, or `cancelled`.
     */
    status: "created" | "running" | "succeeded" | "failed" | "cancelled";
    /**
     * The hyperparameters used for the fine-tuning job. See the
     * [fine-tuning guide](/docs/guides/legacy-fine-tuning/hyperparameters) for more details.
     */
    hyperparams: Record<string, any>;
    /** The list of files used for training. */
    training_files: Array<OpenAIFileOutput>;
    /** The list of files used for validation. */
    validation_files: Array<OpenAIFileOutput>;
    /** The compiled results files for the fine-tuning job. */
    result_files: Array<OpenAIFileOutput>;
    /** The list of events that have been observed in the lifecycle of the FineTune job. */
    events?: Array<FineTuneEventOutput>;
}

export declare interface FineTunesCancel {
    post(options?: FineTunesCancelParameters): StreamableMethod<FineTunesCancel200Response | FineTunesCancelDefaultResponse>;
}

/** The request has succeeded. */
export declare interface FineTunesCancel200Response extends HttpResponse {
    status: "200";
    body: FineTuneOutput;
}

export declare interface FineTunesCancelDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FineTunesCancelParameters = RequestParameters;

export declare interface FineTunesCreate {
    post(options: FineTunesCreateParameters): StreamableMethod<FineTunesCreate200Response | FineTunesCreateDefaultResponse>;
    get(options?: FineTunesListParameters): StreamableMethod<FineTunesList200Response | FineTunesListDefaultResponse>;
}

/** The request has succeeded. */
export declare interface FineTunesCreate200Response extends HttpResponse {
    status: "200";
    body: FineTuneOutput;
}

export declare interface FineTunesCreateBodyParam {
    body: CreateFineTuneRequest;
}

export declare interface FineTunesCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FineTunesCreateParameters = FineTunesCreateBodyParam & RequestParameters;

/** The request has succeeded. */
export declare interface FineTunesList200Response extends HttpResponse {
    status: "200";
    body: ListFineTunesResponseOutput;
}

export declare interface FineTunesListDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface FineTunesListEvents {
    get(options?: FineTunesListEventsParameters): StreamableMethod<FineTunesListEvents200Response | FineTunesListEventsDefaultResponse>;
}

/** The request has succeeded. */
export declare interface FineTunesListEvents200Response extends HttpResponse {
    status: "200";
    body: ListFineTuneEventsResponseOutput;
}

export declare interface FineTunesListEventsDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FineTunesListEventsParameters = FineTunesListEventsQueryParam & RequestParameters;

export declare interface FineTunesListEventsQueryParam {
    queryParameters?: FineTunesListEventsQueryParamProperties;
}

export declare interface FineTunesListEventsQueryParamProperties {
    /**
     * Whether to stream events for the fine-tune job. If set to true, events will be sent as
     * data-only
     * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
     * as they become available. The stream will terminate with a `data: [DONE]` message when the
     * job is finished (succeeded, cancelled, or failed).
     *
     * If set to false, only events generated so far will be returned.
     */
    stream?: boolean;
}

export declare type FineTunesListParameters = RequestParameters;

export declare interface FineTunesRetrieve {
    get(options?: FineTunesRetrieveParameters): StreamableMethod<FineTunesRetrieve200Response | FineTunesRetrieveDefaultResponse>;
}

/** The request has succeeded. */
export declare interface FineTunesRetrieve200Response extends HttpResponse {
    status: "200";
    body: FineTuneOutput;
}

export declare interface FineTunesRetrieveDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type FineTunesRetrieveParameters = RequestParameters;

export declare interface FineTuningJobEventOutput {
    object: string;
    created_at: number;
    level: "info" | "warn" | "error";
    message: string;
}

export declare interface FineTuningJobOutput {
    /** The object identifier, which can be referenced in the API endpoints. */
    id: string;
    /** The object type, which is always "fine_tuning.job". */
    object: "fine_tuning.job";
    /** The Unix timestamp (in seconds) for when the fine-tuning job was created. */
    created_at: number;
    /** The Unix timestamp (in seconds) for when the fine-tuning job was finished. */
    finished_at: number;
    /** The base model that is being fine-tuned. */
    model: string;
    /** The name of the fine-tuned model that is being created. */
    fine_tuned_model: string | null;
    /** The organization that owns the fine-tuning job. */
    organization_id: string;
    /**
     * The current status of the fine-tuning job, which can be either `created`, `pending`, `running`,
     * `succeeded`, `failed`, or `cancelled`.
     */
    status: "created" | "pending" | "running" | "succeeded" | "failed" | "cancelled";
    /**
     * The hyperparameters used for the fine-tuning job. See the
     * [fine-tuning guide](/docs/guides/fine-tuning) for more details.
     */
    hyperparameters: Record<string, any>;
    /** The file ID used for training. */
    training_file: string;
    /** The file ID used for validation. */
    validation_file: string | null;
    /** The compiled results files for the fine-tuning job. */
    result_files: Array<OpenAIFileOutput>;
    /** The total number of billable tokens processed by this fine tuning job. */
    trained_tokens: number;
}

/** Represents the url or the content of an image generated by the OpenAI API. */
export declare interface ImageOutput {
    /** The URL of the generated image, if `response_format` is `url` (default). */
    url?: string;
    /** The base64-encoded JSON of the generated image, if `response_format` is `b64_json`. */
    b64_json?: string;
}

export declare interface ImagesCreate {
    post(options: ImagesCreateParameters): StreamableMethod<ImagesCreate200Response | ImagesCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface ImagesCreate200Response extends HttpResponse {
    status: "200";
    body: ImagesResponseOutput;
}

export declare interface ImagesCreateBodyParam {
    body: CreateImageRequest;
}

export declare interface ImagesCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface ImagesCreateEdit {
    post(options: ImagesCreateEditParameters): StreamableMethod<ImagesCreateEdit200Response | ImagesCreateEditDefaultResponse>;
}

/** The request has succeeded. */
export declare interface ImagesCreateEdit200Response extends HttpResponse {
    status: "200";
    body: ImagesResponseOutput;
}

export declare interface ImagesCreateEditBodyParam {
    body: ImagesCreateEditFormBody;
}

export declare interface ImagesCreateEditDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface ImagesCreateEditFormBody {
    prompt: string;
    image: string;
    mask?: string;
    n?: number | null;
    size?: "256x256" | "512x512" | "1024x1024" | null;
    response_format?: "url" | "b64_json" | null;
    user?: string;
}

export declare interface ImagesCreateEditMediaTypesParam {
    contentType: "multipart/form-data";
}

export declare type ImagesCreateEditParameters = ImagesCreateEditMediaTypesParam & ImagesCreateEditBodyParam & RequestParameters;

export declare type ImagesCreateParameters = ImagesCreateBodyParam & RequestParameters;

export declare interface ImagesCreateVariation {
    post(options: ImagesCreateVariationParameters): StreamableMethod<ImagesCreateVariation200Response | ImagesCreateVariationDefaultResponse>;
}

/** The request has succeeded. */
export declare interface ImagesCreateVariation200Response extends HttpResponse {
    status: "200";
    body: ImagesResponseOutput;
}

export declare interface ImagesCreateVariationBodyParam {
    body: ImagesCreateVariationFormBody;
}

export declare interface ImagesCreateVariationDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface ImagesCreateVariationFormBody {
    image: string;
    n?: number | null;
    size?: "256x256" | "512x512" | "1024x1024" | null;
    response_format?: "url" | "b64_json" | null;
    user?: string;
}

export declare interface ImagesCreateVariationMediaTypesParam {
    contentType: "multipart/form-data";
}

export declare type ImagesCreateVariationParameters = ImagesCreateVariationMediaTypesParam & ImagesCreateVariationBodyParam & RequestParameters;

export declare interface ImagesResponseOutput {
    created: number;
    data: Array<ImageOutput>;
}

export declare function isUnexpected(response: CompletionsCreate200Response | CompletionsCreateDefaultResponse): response is CompletionsCreateDefaultResponse;

export declare function isUnexpected(response: TranscriptionsCreate200Response | TranscriptionsCreateDefaultResponse): response is TranscriptionsCreateDefaultResponse;

export declare function isUnexpected(response: TranslationsCreate200Response | TranslationsCreateDefaultResponse): response is TranslationsCreateDefaultResponse;

export declare function isUnexpected(response: JobsCreate200Response | JobsCreateDefaultResponse): response is JobsCreateDefaultResponse;

export declare function isUnexpected(response: JobsList200Response | JobsListDefaultResponse): response is JobsListDefaultResponse;

export declare function isUnexpected(response: JobsRetrieve200Response | JobsRetrieveDefaultResponse): response is JobsRetrieveDefaultResponse;

export declare function isUnexpected(response: JobsListEvents200Response | JobsListEventsDefaultResponse): response is JobsListEventsDefaultResponse;

export declare function isUnexpected(response: JobsCancel200Response | JobsCancelDefaultResponse): response is JobsCancelDefaultResponse;

export declare function isUnexpected(response: CompletionsCreate200Response | CompletionsCreateDefaultResponse): response is CompletionsCreateDefaultResponse;

export declare function isUnexpected(response: EditsCreate200Response | EditsCreateDefaultResponse): response is EditsCreateDefaultResponse;

export declare function isUnexpected(response: ImagesCreate200Response | ImagesCreateDefaultResponse): response is ImagesCreateDefaultResponse;

export declare function isUnexpected(response: ImagesCreateEdit200Response | ImagesCreateEditDefaultResponse): response is ImagesCreateEditDefaultResponse;

export declare function isUnexpected(response: ImagesCreateVariation200Response | ImagesCreateVariationDefaultResponse): response is ImagesCreateVariationDefaultResponse;

export declare function isUnexpected(response: EmbeddingsCreate200Response | EmbeddingsCreateDefaultResponse): response is EmbeddingsCreateDefaultResponse;

export declare function isUnexpected(response: FilesList200Response | FilesListDefaultResponse): response is FilesListDefaultResponse;

export declare function isUnexpected(response: FilesCreate200Response | FilesCreateDefaultResponse): response is FilesCreateDefaultResponse;

export declare function isUnexpected(response: FilesRetrieve200Response | FilesRetrieveDefaultResponse): response is FilesRetrieveDefaultResponse;

export declare function isUnexpected(response: FilesDeleteOperation200Response | FilesDeleteOperationDefaultResponse): response is FilesDeleteOperationDefaultResponse;

export declare function isUnexpected(response: FilesDownload200Response | FilesDownloadDefaultResponse): response is FilesDownloadDefaultResponse;

export declare function isUnexpected(response: FineTunesCreate200Response | FineTunesCreateDefaultResponse): response is FineTunesCreateDefaultResponse;

export declare function isUnexpected(response: FineTunesList200Response | FineTunesListDefaultResponse): response is FineTunesListDefaultResponse;

export declare function isUnexpected(response: FineTunesRetrieve200Response | FineTunesRetrieveDefaultResponse): response is FineTunesRetrieveDefaultResponse;

export declare function isUnexpected(response: FineTunesListEvents200Response | FineTunesListEventsDefaultResponse): response is FineTunesListEventsDefaultResponse;

export declare function isUnexpected(response: FineTunesCancel200Response | FineTunesCancelDefaultResponse): response is FineTunesCancelDefaultResponse;

export declare function isUnexpected(response: ModelsList200Response | ModelsListDefaultResponse): response is ModelsListDefaultResponse;

export declare function isUnexpected(response: ModelsRetrieve200Response | ModelsRetrieveDefaultResponse): response is ModelsRetrieveDefaultResponse;

export declare function isUnexpected(response: ModelsDeleteOperation200Response | ModelsDeleteOperationDefaultResponse): response is ModelsDeleteOperationDefaultResponse;

export declare function isUnexpected(response: ModerationsCreate200Response | ModerationsCreateDefaultResponse): response is ModerationsCreateDefaultResponse;

export declare interface JobsCancel {
    post(options?: JobsCancelParameters): StreamableMethod<JobsCancel200Response | JobsCancelDefaultResponse>;
}

/** The request has succeeded. */
export declare interface JobsCancel200Response extends HttpResponse {
    status: "200";
    body: FineTuningJobOutput;
}

export declare interface JobsCancelDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type JobsCancelParameters = RequestParameters;

export declare interface JobsCreate {
    /**
     * Creates a job that fine-tunes a specified model from a given dataset.
     *
     * Response includes details of the enqueued job including job status and the name of the
     * fine-tuned models once complete.
     *
     * [Learn more about fine-tuning](/docs/guides/fine-tuning)
     */
    post(options: JobsCreateParameters): StreamableMethod<JobsCreate200Response | JobsCreateDefaultResponse>;
    get(options?: JobsListParameters): StreamableMethod<JobsList200Response | JobsListDefaultResponse>;
}

/** The request has succeeded. */
export declare interface JobsCreate200Response extends HttpResponse {
    status: "200";
    body: FineTuningJobOutput;
}

export declare interface JobsCreateBodyParam {
    body: CreateFineTuningJobRequest;
}

export declare interface JobsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type JobsCreateParameters = JobsCreateBodyParam & RequestParameters;

/** The request has succeeded. */
export declare interface JobsList200Response extends HttpResponse {
    status: "200";
    body: ListPaginatedFineTuningJobsResponseOutput;
}

export declare interface JobsListDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface JobsListEvents {
    get(options?: JobsListEventsParameters): StreamableMethod<JobsListEvents200Response | JobsListEventsDefaultResponse>;
}

/** The request has succeeded. */
export declare interface JobsListEvents200Response extends HttpResponse {
    status: "200";
    body: ListFineTuningJobEventsResponseOutput;
}

export declare interface JobsListEventsDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type JobsListEventsParameters = JobsListEventsQueryParam & RequestParameters;

export declare interface JobsListEventsQueryParam {
    queryParameters?: JobsListEventsQueryParamProperties;
}

export declare interface JobsListEventsQueryParamProperties {
    /** Identifier for the last event from the previous pagination request. */
    after?: string;
    /** Number of events to retrieve. */
    limit?: number;
}

export declare type JobsListParameters = JobsListQueryParam & RequestParameters;

export declare interface JobsListQueryParam {
    queryParameters?: JobsListQueryParamProperties;
}

export declare interface JobsListQueryParamProperties {
    /** Identifier for the last job from the previous pagination request. */
    after?: string;
    /** Number of fine-tuning jobs to retrieve. */
    limit?: number;
}

export declare interface JobsRetrieve {
    get(options?: JobsRetrieveParameters): StreamableMethod<JobsRetrieve200Response | JobsRetrieveDefaultResponse>;
}

/** The request has succeeded. */
export declare interface JobsRetrieve200Response extends HttpResponse {
    status: "200";
    body: FineTuningJobOutput;
}

export declare interface JobsRetrieveDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type JobsRetrieveParameters = RequestParameters;

export declare interface ListFilesResponseOutput {
    object: string;
    data: Array<OpenAIFileOutput>;
}

export declare interface ListFineTuneEventsResponseOutput {
    object: string;
    data: Array<FineTuneEventOutput>;
}

export declare interface ListFineTunesResponseOutput {
    object: string;
    data: Array<FineTuneOutput>;
}

export declare interface ListFineTuningJobEventsResponseOutput {
    object: string;
    data: Array<FineTuningJobEventOutput>;
}

export declare interface ListModelsResponseOutput {
    object: string;
    data: Array<ModelOutput>;
}

export declare interface ListPaginatedFineTuningJobsResponseOutput {
    object: string;
    data: Array<FineTuningJobOutput>;
    has_more: boolean;
}

/** Describes an OpenAI model offering that can be used with the API. */
export declare interface ModelOutput {
    /** The model identifier, which can be referenced in the API endpoints. */
    id: string;
    /** The object type, which is always "model". */
    object: "model";
    /** The Unix timestamp (in seconds) when the model was created. */
    created: number;
    /** The organization that owns the model. */
    owned_by: string;
}

/** The request has succeeded. */
export declare interface ModelsDeleteOperation200Response extends HttpResponse {
    status: "200";
    body: DeleteModelResponseOutput;
}

export declare interface ModelsDeleteOperationDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type ModelsDeleteParameters = RequestParameters;

export declare interface ModelsList {
    get(options?: ModelsListParameters): StreamableMethod<ModelsList200Response | ModelsListDefaultResponse>;
}

/** The request has succeeded. */
export declare interface ModelsList200Response extends HttpResponse {
    status: "200";
    body: ListModelsResponseOutput;
}

export declare interface ModelsListDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type ModelsListParameters = RequestParameters;

export declare interface ModelsRetrieve {
    get(options?: ModelsRetrieveParameters): StreamableMethod<ModelsRetrieve200Response | ModelsRetrieveDefaultResponse>;
    delete(options?: ModelsDeleteParameters): StreamableMethod<ModelsDeleteOperation200Response | ModelsDeleteOperationDefaultResponse>;
}

/** The request has succeeded. */
export declare interface ModelsRetrieve200Response extends HttpResponse {
    status: "200";
    body: ModelOutput;
}

export declare interface ModelsRetrieveDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type ModelsRetrieveParameters = RequestParameters;

export declare interface ModerationsCreate {
    post(options: ModerationsCreateParameters): StreamableMethod<ModerationsCreate200Response | ModerationsCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface ModerationsCreate200Response extends HttpResponse {
    status: "200";
    body: CreateModerationResponseOutput;
}

export declare interface ModerationsCreateBodyParam {
    body: CreateModerationRequest;
}

export declare interface ModerationsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare type ModerationsCreateParameters = ModerationsCreateBodyParam & RequestParameters;

export declare type OpenAIClient = Client & {
    path: Routes;
};

/** The `File` object represents a document that has been uploaded to OpenAI. */
export declare interface OpenAIFileOutput {
    /** The file identifier, which can be referenced in the API endpoints. */
    id: string;
    /** The object type, which is always "file". */
    object: "file";
    /** The size of the file in bytes. */
    bytes: number;
    /** The Unix timestamp (in seconds) for when the file was created. */
    createdAt: number;
    /** The name of the file. */
    filename: string;
    /** The intended purpose of the file. Currently, only "fine-tune" is supported. */
    purpose: string;
    /**
     * The current status of the file, which can be either `uploaded`, `processed`, `pending`,
     * `error`, `deleting` or `deleted`.
     */
    status: "uploaded" | "processed" | "pending" | "error" | "deleting" | "deleted";
    /**
     * Additional details about the status of the file. If the file is in the `error` state, this will
     * include a message describing the error.
     */
    status_details?: string | null;
}

export declare interface Routes {
    /** Resource for '/chat/completions' has methods for the following verbs: post */
    (path: "/chat/completions"): CompletionsCreate;
    /** Resource for '/audio/transcriptions' has methods for the following verbs: post */
    (path: "/audio/transcriptions"): TranscriptionsCreate;
    /** Resource for '/audio/translations' has methods for the following verbs: post */
    (path: "/audio/translations"): TranslationsCreate;
    /** Resource for '/fine_tuning/jobs' has methods for the following verbs: post, get */
    (path: "/fine_tuning/jobs"): JobsCreate;
    /** Resource for '/fine_tuning/jobs/\{fine_tuning_job_id\}' has methods for the following verbs: get */
    (path: "/fine_tuning/jobs/{fine_tuning_job_id}", fineTuningJobId: string): JobsRetrieve;
    /** Resource for '/fine_tuning/jobs/\{fine_tuning_job_id\}/events' has methods for the following verbs: get */
    (path: "/fine_tuning/jobs/{fine_tuning_job_id}/events", fineTuningJobId: string): JobsListEvents;
    /** Resource for '/fine_tuning/jobs/\{fine_tuning_job_id\}/cancel' has methods for the following verbs: post */
    (path: "/fine_tuning/jobs/{fine_tuning_job_id}/cancel", fineTuningJobId: string): JobsCancel;
    /** Resource for '/completions' has methods for the following verbs: post */
    (path: "/completions"): CompletionsCreate;
    /** Resource for '/edits' has methods for the following verbs: post */
    (path: "/edits"): EditsCreate;
    /** Resource for '/images/generations' has methods for the following verbs: post */
    (path: "/images/generations"): ImagesCreate;
    /** Resource for '/images/edits' has methods for the following verbs: post */
    (path: "/images/edits"): ImagesCreateEdit;
    /** Resource for '/images/variations' has methods for the following verbs: post */
    (path: "/images/variations"): ImagesCreateVariation;
    /** Resource for '/embeddings' has methods for the following verbs: post */
    (path: "/embeddings"): EmbeddingsCreate;
    /** Resource for '/files' has methods for the following verbs: get, post */
    (path: "/files"): FilesList;
    /** Resource for '/files/files/\{file_id\}' has methods for the following verbs: post, delete */
    (path: "/files/files/{file_id}", fileId: string): FilesRetrieve;
    /** Resource for '/files/files/\{file_id\}/content' has methods for the following verbs: get */
    (path: "/files/files/{file_id}/content", fileId: string): FilesDownload;
    /** Resource for '/fine-tunes' has methods for the following verbs: post, get */
    (path: "/fine-tunes"): FineTunesCreate;
    /** Resource for '/fine-tunes/\{fine_tune_id\}' has methods for the following verbs: get */
    (path: "/fine-tunes/{fine_tune_id}", fineTuneId: string): FineTunesRetrieve;
    /** Resource for '/fine-tunes/\{fine_tune_id\}/events' has methods for the following verbs: get */
    (path: "/fine-tunes/{fine_tune_id}/events", fineTuneId: string): FineTunesListEvents;
    /** Resource for '/fine-tunes/\{fine_tune_id\}/cancel' has methods for the following verbs: post */
    (path: "/fine-tunes/{fine_tune_id}/cancel", fineTuneId: string): FineTunesCancel;
    /** Resource for '/models' has methods for the following verbs: get */
    (path: "/models"): ModelsList;
    /** Resource for '/models/\{model\}' has methods for the following verbs: get, delete */
    (path: "/models/{model}", model: string): ModelsRetrieve;
    /** Resource for '/moderations' has methods for the following verbs: post */
    (path: "/moderations"): ModerationsCreate;
}

export declare interface TranscriptionsCreate {
    post(options: TranscriptionsCreateParameters): StreamableMethod<TranscriptionsCreate200Response | TranscriptionsCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface TranscriptionsCreate200Response extends HttpResponse {
    status: "200";
    body: CreateTranscriptionResponseOutput;
}

export declare interface TranscriptionsCreateBodyParam {
    body: TranscriptionsCreateFormBody;
}

export declare interface TranscriptionsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface TranscriptionsCreateFormBody {
    file: string;
    model: string | "whisper-1";
    prompt?: string;
    response_format?: "json" | "text" | "srt" | "verbose_json" | "vtt";
    temperature?: number;
    language?: string;
}

export declare interface TranscriptionsCreateMediaTypesParam {
    contentType: "multipart/form-data";
}

export declare type TranscriptionsCreateParameters = TranscriptionsCreateMediaTypesParam & TranscriptionsCreateBodyParam & RequestParameters;

export declare interface TranslationsCreate {
    post(options: TranslationsCreateParameters): StreamableMethod<TranslationsCreate200Response | TranslationsCreateDefaultResponse>;
}

/** The request has succeeded. */
export declare interface TranslationsCreate200Response extends HttpResponse {
    status: "200";
    body: CreateTranslationResponseOutput;
}

export declare interface TranslationsCreateBodyParam {
    body: TranslationsCreateFormBody;
}

export declare interface TranslationsCreateDefaultResponse extends HttpResponse {
    status: string;
    body: ErrorResponseOutput;
}

export declare interface TranslationsCreateFormBody {
    file: string;
    model: string | "whisper-1";
    prompt?: string;
    response_format?: "json" | "text" | "srt" | "verbose_json" | "vtt";
    temperature?: number;
}

export declare interface TranslationsCreateMediaTypesParam {
    contentType: "multipart/form-data";
}

export declare type TranslationsCreateParameters = TranslationsCreateMediaTypesParam & TranslationsCreateBodyParam & RequestParameters;

export { }
