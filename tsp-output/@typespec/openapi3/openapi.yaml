openapi: 3.0.0
info:
  title: OpenAI API
  version: 2.0.0
tags: []
paths:
  /chat:
    post:
      operationId: Chat_createChatCompletion
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateChatCompletionRequest'
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: >-
          Returns a [chat completion](/docs/api-reference/chat/object) object,
          or a streamed sequence of

          [chat completion chunk](/docs/api-reference/chat/streaming) objects if
          the request is streamed.
        path: create
        examples:
          - title: No streaming
            request:
              curl: |-
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_model_id",
                  "messages": [
                    {
                      "role": "system",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ]
              python: |-
                import os
                import openai
                openai.api_key = os.getenv("OPENAI_API_KEY")

                completion = openai.ChatCompletion.create(
                  model="VAR_model_id",
                  messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ]
                )

                print(completion.choices[0].message)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    messages: [{ role: "system", content: "string" }],
                    model: "VAR_model_id",
                  });

                  console.log(completion.choices[0]);
                }

                main();
            response: |-
              {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "gpt-3.5-turbo-0613",
                "choices": [{
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "

              Hello there, how may I assist you today?",
                  },
                  "finish_reason": "stop"
                }],
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 12,
                  "total_tokens": 21
                }
              }
          - title: Streaming
            request:
              curl: |-
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_model_id",
                  "messages": [
                    {
                      "role": "system",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ],
                  "stream": true
                }'
              python: |-
                import os
                import openai
                openai.api_key = os.getenv("OPENAI_API_KEY")

                completion = openai.ChatCompletion.create(
                  model="VAR_model_id",
                  messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ],
                  stream=True
                )

                for chunk in completion:
                  print(chunk.choices[0].delta)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    model: "VAR_model_id",
                    messages: [
                      {"role": "system", "content": "You are a helpful assistant."},
                      {"role": "user", "content": "Hello!"}
                    ],
                    stream: true,
                  });

                  for await (const chunk of completion) {
                    console.log(chunk.choices[0].delta.content);
                  }
                }

                main();
            response: |-
              {
                "id": "chatcmpl-123",
                "object": "chat.completion.chunk",
                "created": 1677652288,
                "model": "gpt-3.5-turbo",
                "choices": [{
                  "index": 0,
                  "delta": {
                    "content": "Hello",
                  },
                  "finish_reason": "stop"
                }]
              }
components:
  schemas:
    ChatCompletionFunctionCallOption:
      type: object
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
        - name
    ChatCompletionFunctionParameters:
      type: object
      additionalProperties: {}
    ChatCompletionFunctions:
      type: object
      properties:
        name:
          type: string
          description: >-
            The name of the function to be called. Must be a-z, A-Z, 0-9, or
            contain underscores and

            dashes, with a maximum length of 64.
        description:
          type: string
          description: >-
            A description of what the function does, used by the model to choose
            when and how to call the

            function.
        parameters:
          allOf:
            - $ref: '#/components/schemas/ChatCompletionFunctionParameters'
          description: >-
            The parameters the functions accepts, described as a JSON Schema
            object. See the

            [guide](/docs/guides/gpt/function-calling) for examples, and the

            [JSON Schema
            reference](https://json-schema.org/understanding-json-schema/) for
            documentation

            about the format.\n\nTo describe a function that accepts no
            parameters, provide the value

            `{\"type\": \"object\", \"properties\": {}}`.
      required:
        - name
        - parameters
    ChatCompletionRequestMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - system
            - user
            - assistant
            - function
          description: >-
            The role of the messages author. One of `system`, `user`,
            `assistant`, or `function`.
        content:
          type: string
          nullable: true
          description: >-
            The contents of the message. `content` is required for all messages,
            and may be null for

            assistant messages with function calls.
        name:
          type: string
          description: >-
            The name of the author of this message. `name` is required if role
            is `function`, and it

            should be the name of the function whose response is in the
            `content`. May contain a-z,

            A-Z, 0-9, and underscores, with a maximum length of 64 characters.
        function_call:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the
                model in JSON format. Note that

                the model does not always generate valid JSON, and may
                hallucinate parameters not defined by

                your function schema. Validate the arguments in your code before
                calling your function.
          description: >-
            The name and arguments of a function that should be called, as
            generated by the model.
          required:
            - name
            - arguments
      required:
        - role
        - content
    ChatCompletionResponseMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - system
            - user
            - assistant
            - function
          description: The role of the author of this message.
        content:
          type: string
          nullable: true
          description: The contents of the message.
        function_call:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the
                model in JSON format. Note that

                the model does not always generate valid JSON, and may
                hallucinate parameters not defined by

                your function schema. Validate the arguments in your code before
                calling your function.
          description: >-
            The name and arguments of a function that should be called, as
            generated by the model.
          required:
            - name
            - arguments
      required:
        - role
        - content
    CreateChatCompletionRequest:
      type: object
      properties:
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt4
                - gpt-4-0314
                - gpt-4-0613
                - gpt-4-32k
                - gpt-4-32k-0314
                - gpt-4-32k-0613
                - gpt-3.5-turbo
                - gpt-3.5-turbo-16k
                - gpt-3.5-turbo-0301
                - gpt-3.5-turbo-0613
                - gpt-3.5-turbo-16k-0613
          description: >-
            ID of the model to use. See the [model endpoint
            compatibility](/docs/models/model-endpoint-compatibility)

            table for details on which models work with the Chat API.
          x-oaiTypeLabel: string
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionRequestMessage'
          description: >-
            A list of messages comprising the conversation so far.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
          minItems: 1
        functions:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionFunctions'
          description: A list of functions the model may generate JSON inputs for.
          minItems: 1
          maxItems: 128
        function_call:
          anyOf:
            - type: string
              enum:
                - none
                - auto
            - $ref: '#/components/schemas/ChatCompletionFunctionCallOption'
          description: >-
            Controls how the model responds to function calls. "none" means the
            model does not call a

            function, and responds to the end-user. "auto" means the model can
            pick between an end-user or

            calling a function.  Specifying a particular function via `{"name":\
            "my_function"}` forces the

            model to call that function. "none" is the default when no functions
            are present. "auto" is the

            default if functions are present.
        temperature:
          oneOf:
            - $ref: '#/components/schemas/Temperature'
          nullable: true
          description: '*completions_temperature_description'
        top_p:
          oneOf:
            - $ref: '#/components/schemas/TopP'
          nullable: true
          description: '*completions_top_p_description'
        'n':
          oneOf:
            - $ref: '#/components/schemas/N'
          nullable: true
          description: How many chat completion choices to generate for each input message.
        stream:
          type: boolean
          nullable: true
          description: >-
            If set, partial message deltas will be sent, like in ChatGPT. Tokens
            will be sent as data-only

            [server-sent
            events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)

            as they become available, with the stream terminated by a `data:
            [DONE]` message.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
          default: true
        stop:
          anyOf:
            - type: string
            - $ref: '#/components/schemas/StopSequences'
          nullable: true
          description: Up to 4 sequences where the API will stop generating further tokens.
        max_tokens:
          type: integer
          format: int64
          description: >-
            The maximum number of [tokens](/tokenizer) to generate in the chat
            completion.


            The total length of input tokens and generated tokens is limited by
            the model's context length.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)

            for counting tokens.
        presence_penalty:
          oneOf:
            - $ref: '#/components/schemas/Penalty'
          nullable: true
          description: '*completions_presence_penalty_description'
        frequency_penalty:
          oneOf:
            - $ref: '#/components/schemas/Penalty'
          nullable: true
          description: '*completions_frequency_penalty_description'
        logit_bias:
          type: object
          additionalProperties:
            type: integer
            format: int64
          nullable: true
          description: >-
            Modify the likelihood of specified tokens appearing in the
            completion.

            Accepts a json object that maps tokens (specified by their token ID
            in the tokenizer) to an

            associated bias value from -100 to 100. Mathematically, the bias is
            added to the logits

            generated by the model prior to sampling. The exact effect will vary
            per model, but values

            between -1 and 1 should decrease or increase likelihood of
            selection; values like -100 or 100

            should result in a ban or exclusive selection of the relevant token.
          x-oaiTypeLabel: map
        user:
          $ref: '#/components/schemas/User'
      required:
        - model
        - messages
        - temperature
    CreateChatCompletionResponse:
      type: object
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
        created:
          type: integer
          format: unixtime
          description: >-
            The Unix timestamp (in seconds) of when the chat completion was
            created.
        model:
          type: string
          description: The model used for the chat completion.
        choices:
          type: object
          properties:
            index:
              type: integer
              format: int64
              description: The index of the choice in the list of choices.
            message:
              $ref: '#/components/schemas/ChatCompletionResponseMessage'
            finish_reason:
              type: string
              enum:
                - stop
                - length
                - function_call
              description: >-
                The reason the model stopped generating tokens. This will be
                `stop` if the model hit a

                natural stop point or a provided stop sequence, `length` if the
                maximum number of tokens

                specified in the request was reached, or `function_call` if the
                model called a function.
          description: >-
            A list of chat completion choices. Can be more than one if `n` is
            greater than 1.
          required:
            - index
            - message
            - finish_reason
        usage:
          type: string
      description: >-
        Represents a chat completion response returned by model, based on the
        provided input.
      required:
        - id
        - object
        - created
        - model
        - choices
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: ''
    'N':
      type: integer
      format: int64
      minimum: 1
      maximum: 128
    Penalty:
      type: number
      format: double
      minimum: -2
      maximum: 2
    StopSequences:
      type: array
      items:
        type: string
      minItems: 1
      maxItems: 4
    Temperature:
      type: number
      format: double
      minimum: 0
      maximum: 1
    TopP:
      type: number
      format: double
      minimum: 0
      maximum: 1
    User:
      type: string
      description: >-
        A unique identifier representing your end-user, which can help OpenAI to
        monitor and detect

        abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
