openapi: 3.0.0
info:
  title: OpenAI API
  version: 2.0.0
tags:
  - name: OpenAI
paths:
  /chat/completions:
    post:
      tags:
        - OpenAI
      operationId: createChatCompletion
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateChatCompletionRequest'
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: >-
          Returns a [chat completion](/docs/api-reference/chat/object) object,
          or a streamed sequence of

          [chat completion chunk](/docs/api-reference/chat/streaming) objects if
          the request is streamed.
        path: create
        examples:
          - title: No streaming
            request:
              curl: |-
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_model_id",
                  "messages": [
                    {
                      "role": "system",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ]
              python: |-
                import os
                import openai
                openai.api_key = os.getenv("OPENAI_API_KEY")

                completion = openai.ChatCompletion.create(
                  model="VAR_model_id",
                  messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ]
                )

                print(completion.choices[0].message)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    messages: [{ role: "system", content: "string" }],
                    model: "VAR_model_id",
                  });

                  console.log(completion.choices[0]);
                }

                main();
            response: |-
              {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "gpt-3.5-turbo-0613",
                "choices": [{
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "

              Hello there, how may I assist you today?",
                  },
                  "finish_reason": "stop"
                }],
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 12,
                  "total_tokens": 21
                }
              }
          - title: Streaming
            request:
              curl: |-
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_model_id",
                  "messages": [
                    {
                      "role": "system",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ],
                  "stream": true
                }'
              python: |-
                import os
                import openai
                openai.api_key = os.getenv("OPENAI_API_KEY")

                completion = openai.ChatCompletion.create(
                  model="VAR_model_id",
                  messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ],
                  stream=True
                )

                for chunk in completion:
                  print(chunk.choices[0].delta)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    model: "VAR_model_id",
                    messages: [
                      {"role": "system", "content": "You are a helpful assistant."},
                      {"role": "user", "content": "Hello!"}
                    ],
                    stream: true,
                  });

                  for await (const chunk of completion) {
                    console.log(chunk.choices[0].delta.content);
                  }
                }

                main();
            response: |-
              {
                "id": "chatcmpl-123",
                "object": "chat.completion.chunk",
                "created": 1677652288,
                "model": "gpt-3.5-turbo",
                "choices": [{
                  "index": 0,
                  "delta": {
                    "content": "Hello",
                  },
                  "finish_reason": "stop"
                }]
              }
  /completions:
    post:
      tags:
        - OpenAI
      operationId: createCompletion
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateCompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateCompletionRequest'
  /edits:
    post:
      tags:
        - OpenAI
      operationId: createEdit
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateEditResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateEditRequest'
      deprecated: true
components:
  schemas:
    ChatCompletionFunctionCallOption:
      type: object
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
        - name
    ChatCompletionFunctionParameters:
      type: object
      additionalProperties: {}
    ChatCompletionFunctions:
      type: object
      properties:
        name:
          type: string
          description: >-
            The name of the function to be called. Must be a-z, A-Z, 0-9, or
            contain underscores and

            dashes, with a maximum length of 64.
        description:
          type: string
          description: >-
            A description of what the function does, used by the model to choose
            when and how to call the

            function.
        parameters:
          allOf:
            - $ref: '#/components/schemas/ChatCompletionFunctionParameters'
          description: >-
            The parameters the functions accepts, described as a JSON Schema
            object. See the

            [guide](/docs/guides/gpt/function-calling) for examples, and the

            [JSON Schema
            reference](https://json-schema.org/understanding-json-schema/) for
            documentation

            about the format.\n\nTo describe a function that accepts no
            parameters, provide the value

            `{\"type\": \"object\", \"properties\": {}}`.
      required:
        - name
        - parameters
    ChatCompletionRequestMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - system
            - user
            - assistant
            - function
          description: >-
            The role of the messages author. One of `system`, `user`,
            `assistant`, or `function`.
        content:
          type: string
          nullable: true
          description: >-
            The contents of the message. `content` is required for all messages,
            and may be null for

            assistant messages with function calls.
        name:
          type: string
          description: >-
            The name of the author of this message. `name` is required if role
            is `function`, and it

            should be the name of the function whose response is in the
            `content`. May contain a-z,

            A-Z, 0-9, and underscores, with a maximum length of 64 characters.
        function_call:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the
                model in JSON format. Note that

                the model does not always generate valid JSON, and may
                hallucinate parameters not defined by

                your function schema. Validate the arguments in your code before
                calling your function.
          description: >-
            The name and arguments of a function that should be called, as
            generated by the model.
          required:
            - name
            - arguments
      required:
        - role
        - content
    ChatCompletionResponseMessage:
      type: object
      properties:
        role:
          type: string
          enum:
            - system
            - user
            - assistant
            - function
          description: The role of the author of this message.
        content:
          type: string
          nullable: true
          description: The contents of the message.
        function_call:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the
                model in JSON format. Note that

                the model does not always generate valid JSON, and may
                hallucinate parameters not defined by

                your function schema. Validate the arguments in your code before
                calling your function.
          description: >-
            The name and arguments of a function that should be called, as
            generated by the model.
          required:
            - name
            - arguments
      required:
        - role
        - content
    CompletionUsage:
      type: object
      properties:
        prompt_tokens:
          type: integer
          format: int64
          description: Number of tokens in the prompt.
        completion_tokens:
          type: integer
          format: int64
          description: Number of tokens in the generated completion
        total_tokens:
          type: integer
          format: int64
          description: Total number of tokens used in the request (prompt + completion).
      description: Usage statistics for the completion request.
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
    CreateChatCompletionRequest:
      type: object
      properties:
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt4
                - gpt-4-0314
                - gpt-4-0613
                - gpt-4-32k
                - gpt-4-32k-0314
                - gpt-4-32k-0613
                - gpt-3.5-turbo
                - gpt-3.5-turbo-16k
                - gpt-3.5-turbo-0301
                - gpt-3.5-turbo-0613
                - gpt-3.5-turbo-16k-0613
          description: >-
            ID of the model to use. See the [model endpoint
            compatibility](/docs/models/model-endpoint-compatibility)

            table for details on which models work with the Chat API.
          x-oaiTypeLabel: string
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionRequestMessage'
          description: >-
            A list of messages comprising the conversation so far.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
          minItems: 1
        functions:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionFunctions'
          description: A list of functions the model may generate JSON inputs for.
          minItems: 1
          maxItems: 128
        function_call:
          anyOf:
            - type: string
              enum:
                - none
                - auto
            - $ref: '#/components/schemas/ChatCompletionFunctionCallOption'
          description: >-
            Controls how the model responds to function calls. "none" means the
            model does not call a

            function, and responds to the end-user. "auto" means the model can
            pick between an end-user or

            calling a function.  Specifying a particular function via `{"name":\
            "my_function"}` forces the

            model to call that function. "none" is the default when no functions
            are present. "auto" is the

            default if functions are present.
        temperature:
          oneOf:
            - $ref: '#/components/schemas/Temperature'
          nullable: true
          description: >-
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output

            more random, while lower values like 0.2 will make it more focused
            and deterministic.


            We generally recommend altering this or `top_p` but not both.
        top_p:
          oneOf:
            - $ref: '#/components/schemas/TopP'
          nullable: true
          description: >-
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers

            the results of the tokens with top_p probability mass. So 0.1 means
            only the tokens comprising

            the top 10% probability mass are considered.


            We generally recommend altering this or `temperature` but not both.
        'n':
          oneOf:
            - $ref: '#/components/schemas/N'
          nullable: true
          description: >-
            How many completions to generate for each prompt.

            **Note:** Because this parameter generates many completions, it can
            quickly consume your token

            quota. Use carefully and ensure that you have reasonable settings
            for `max_tokens` and `stop`.
        max_tokens:
          oneOf:
            - $ref: '#/components/schemas/MaxTokens'
          nullable: true
          description: >-
            The maximum number of [tokens](/tokenizer) to generate in the
            completion.


            The token count of your prompt plus `max_tokens` cannot exceed the
            model's context length.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)

            for counting tokens.
        stop:
          anyOf:
            - type: string
            - $ref: '#/components/schemas/StopSequences'
          nullable: true
          description: Up to 4 sequences where the API will stop generating further tokens.
        presence_penalty:
          oneOf:
            - $ref: '#/components/schemas/Penalty'
          nullable: true
          description: >-
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on whether they appear

            in the text so far, increasing the model's likelihood to talk about
            new topics.


            [See more information about frequency and presence
            penalties.](/docs/guides/gpt/parameter-details)
        frequency_penalty:
          oneOf:
            - $ref: '#/components/schemas/Penalty'
          nullable: true
          description: >-
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on their existing

            frequency in the text so far, decreasing the model's likelihood to
            repeat the same line

            verbatim.


            [See more information about frequency and presence
            penalties.](/docs/guides/gpt/parameter-details)
        logit_bias:
          type: object
          additionalProperties:
            type: integer
            format: int64
          nullable: true
          description: >-
            Modify the likelihood of specified tokens appearing in the
            completion.

            Accepts a json object that maps tokens (specified by their token ID
            in the tokenizer) to an

            associated bias value from -100 to 100. Mathematically, the bias is
            added to the logits

            generated by the model prior to sampling. The exact effect will vary
            per model, but values

            between -1 and 1 should decrease or increase likelihood of
            selection; values like -100 or 100

            should result in a ban or exclusive selection of the relevant token.
          x-oaiTypeLabel: map
        user:
          allOf:
            - $ref: '#/components/schemas/User'
          description: >-
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect

            abuse. [Learn
            more](/docs/guides/safety-best-practices/end-user-ids).
        stream:
          type: boolean
          nullable: true
          description: >-
            If set, partial message deltas will be sent, like in ChatGPT. Tokens
            will be sent as data-only

            [server-sent
            events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)

            as they become available, with the stream terminated by a `data:
            [DONE]` message.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
          default: true
      required:
        - model
        - messages
    CreateChatCompletionResponse:
      type: object
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
        created:
          type: integer
          format: unixtime
          description: >-
            The Unix timestamp (in seconds) of when the chat completion was
            created.
        model:
          type: string
          description: The model used for the chat completion.
        choices:
          type: array
          items:
            type: object
            properties:
              index:
                type: integer
                format: int64
                description: The index of the choice in the list of choices.
              message:
                $ref: '#/components/schemas/ChatCompletionResponseMessage'
              finish_reason:
                type: string
                enum:
                  - stop
                  - length
                  - function_call
                description: >-
                  The reason the model stopped generating tokens. This will be
                  `stop` if the model hit a

                  natural stop point or a provided stop sequence, `length` if
                  the maximum number of tokens

                  specified in the request was reached, or `function_call` if
                  the model called a function.
            required:
              - index
              - message
              - finish_reason
          description: >-
            A list of chat completion choices. Can be more than one if `n` is
            greater than 1.
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      description: >-
        Represents a chat completion response returned by model, based on the
        provided input.
      required:
        - id
        - object
        - created
        - model
        - choices
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: ''
    CreateCompletionRequest:
      type: object
      properties:
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - babbage-002
                - davinci-002
                - text-davinci-003
                - text-davinci-002
                - text-davinci-001
                - code-davinci-002
                - text-curie-001
                - text-babbage-001
                - text-ada-001
          description: >-
            ID of the model to use. You can use the [List
            models](/docs/api-reference/models/list) API to

            see all of your available models, or see our [Model
            overview](/docs/models/overview) for

            descriptions of them.
          x-oaiTypeLabel: string
        prompt:
          anyOf:
            - type: string
            - type: array
              items:
                type: string
            - $ref: '#/components/schemas/TokenArray'
            - $ref: '#/components/schemas/TokenArrayArray'
          nullable: true
          description: >-
            The prompt(s) to generate completions for, encoded as a string,
            array of strings, array of

            tokens, or array of token arrays.


            Note that <|endoftext|> is the document separator that the model
            sees during training, so if a

            prompt is not specified the model will generate as if from the
            beginning of a new document.
          default: <|endoftext|>
        suffix:
          type: string
          nullable: true
          description: The suffix that comes after a completion of inserted text.
        temperature:
          oneOf:
            - $ref: '#/components/schemas/Temperature'
          nullable: true
          description: >-
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output

            more random, while lower values like 0.2 will make it more focused
            and deterministic.


            We generally recommend altering this or `top_p` but not both.
        top_p:
          oneOf:
            - $ref: '#/components/schemas/TopP'
          nullable: true
          description: >-
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers

            the results of the tokens with top_p probability mass. So 0.1 means
            only the tokens comprising

            the top 10% probability mass are considered.


            We generally recommend altering this or `temperature` but not both.
        'n':
          oneOf:
            - $ref: '#/components/schemas/N'
          nullable: true
          description: >-
            How many completions to generate for each prompt.

            **Note:** Because this parameter generates many completions, it can
            quickly consume your token

            quota. Use carefully and ensure that you have reasonable settings
            for `max_tokens` and `stop`.
        max_tokens:
          oneOf:
            - $ref: '#/components/schemas/MaxTokens'
          nullable: true
          description: >-
            The maximum number of [tokens](/tokenizer) to generate in the
            completion.


            The token count of your prompt plus `max_tokens` cannot exceed the
            model's context length.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)

            for counting tokens.
        stop:
          anyOf:
            - type: string
            - $ref: '#/components/schemas/StopSequences'
          nullable: true
          description: Up to 4 sequences where the API will stop generating further tokens.
        presence_penalty:
          oneOf:
            - $ref: '#/components/schemas/Penalty'
          nullable: true
          description: >-
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on whether they appear

            in the text so far, increasing the model's likelihood to talk about
            new topics.


            [See more information about frequency and presence
            penalties.](/docs/guides/gpt/parameter-details)
        frequency_penalty:
          oneOf:
            - $ref: '#/components/schemas/Penalty'
          nullable: true
          description: >-
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on their existing

            frequency in the text so far, decreasing the model's likelihood to
            repeat the same line

            verbatim.


            [See more information about frequency and presence
            penalties.](/docs/guides/gpt/parameter-details)
        logit_bias:
          type: object
          additionalProperties:
            type: integer
            format: int64
          nullable: true
          description: >-
            Modify the likelihood of specified tokens appearing in the
            completion.

            Accepts a json object that maps tokens (specified by their token ID
            in the tokenizer) to an

            associated bias value from -100 to 100. Mathematically, the bias is
            added to the logits

            generated by the model prior to sampling. The exact effect will vary
            per model, but values

            between -1 and 1 should decrease or increase likelihood of
            selection; values like -100 or 100

            should result in a ban or exclusive selection of the relevant token.
          x-oaiTypeLabel: map
        user:
          allOf:
            - $ref: '#/components/schemas/User'
          description: >-
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect

            abuse. [Learn
            more](/docs/guides/safety-best-practices/end-user-ids).
        stream:
          type: boolean
          nullable: true
          description: >-
            If set, partial message deltas will be sent, like in ChatGPT. Tokens
            will be sent as data-only

            [server-sent
            events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)

            as they become available, with the stream terminated by a `data:
            [DONE]` message.

            [Example Python
            code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
          default: true
        logprobs:
          type: integer
          format: int64
          nullable: true
          description: >-
            Include the log probabilities on the `logprobs` most likely tokens,
            as well the chosen tokens.

            For example, if `logprobs` is 5, the API will return a list of the 5
            most likely tokens. The

            API will always return the `logprob` of the sampled token, so there
            may be up to `logprobs+1`

            elements in the response.


            The maximum value for `logprobs` is 5.
        echo:
          type: boolean
          nullable: true
          description: Echo back the prompt in addition to the completion
          default: false
        best_of:
          type: integer
          format: int64
          nullable: true
          description: >-
            Generates `best_of` completions server-side and returns the "best"
            (the one with the highest

            log probability per token). Results cannot be streamed.


            When used with `n`, `best_of` controls the number of candidate
            completions and `n` specifies

            how many to return – `best_of` must be greater than `n`.


            **Note:** Because this parameter generates many completions, it can
            quickly consume your token

            quota. Use carefully and ensure that you have reasonable settings
            for `max_tokens` and `stop`.
      required:
        - model
        - prompt
        - best_of
    CreateCompletionResponse:
      type: object
      properties:
        id:
          type: string
          description: A unique identifier for the completion.
        object:
          type: string
          description: The object type, which is always `text_completion`.
        created:
          type: integer
          format: unixtime
          description: The Unix timestamp (in seconds) of when the completion was created.
        model:
          type: string
          description: The model used for the completion.
        choices:
          type: array
          items:
            type: object
            properties:
              index:
                type: integer
                format: int64
              text:
                type: string
              logprobs:
                type: object
                properties:
                  tokens:
                    type: array
                    items:
                      type: string
                  token_logprobs:
                    type: array
                    items:
                      type: number
                      format: double
                  top_logprobs:
                    type: array
                    items:
                      type: object
                      additionalProperties:
                        type: integer
                        format: int64
                  text_offset:
                    type: array
                    items:
                      type: integer
                      format: int64
                required:
                  - tokens
                  - token_logprobs
                  - top_logprobs
                  - text_offset
                nullable: true
              finish_reason:
                type: string
                enum:
                  - stop
                  - length
                description: >-
                  The reason the model stopped generating tokens. This will be
                  `stop` if the model hit a

                  natural stop point or a provided stop sequence, or `length` if
                  the maximum number of tokens

                  specified in the request was reached.
            required:
              - index
              - text
              - logprobs
              - finish_reason
          description: The list of completion choices the model generated for the input.
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      description: >-
        Represents a completion response from the API. Note: both the streamed
        and non-streamed response

        objects share the same shape (unlike the chat endpoint).
      required:
        - id
        - object
        - created
        - model
        - choices
      x-oaiMeta:
        name: The  completion object
        legacy: true
        example: ''
    CreateEditRequest:
      type: object
      properties:
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - text-davinci-edit-001
                - code-davinci-edit-001
          description: >-
            ID of the model to use. You can use the `text-davinci-edit-001` or
            `code-davinci-edit-001`

            model with this endpoint.
          x-oaiTypeLabel: string
        input:
          type: string
          nullable: true
          description: The input text to use as a starting point for the edit.
          default: ''
        instruction:
          type: string
          description: The instruction that tells the model how to edit the prompt.
        'n':
          oneOf:
            - $ref: '#/components/schemas/EditN'
          nullable: true
          description: How many edits to generate for the input and instruction.
        temperature:
          oneOf:
            - $ref: '#/components/schemas/Temperature'
          nullable: true
          description: >-
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output

            more random, while lower values like 0.2 will make it more focused
            and deterministic.


            We generally recommend altering this or `top_p` but not both.
        top_p:
          oneOf:
            - $ref: '#/components/schemas/TopP'
          nullable: true
          description: >-
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers

            the results of the tokens with top_p probability mass. So 0.1 means
            only the tokens comprising

            the top 10% probability mass are considered.


            We generally recommend altering this or `temperature` but not both.
      required:
        - model
        - instruction
    CreateEditResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - edit
          description: The object type, which is always `edit`.
        created:
          type: integer
          format: unixtime
          description: The Unix timestamp (in seconds) of when the edit was created.
        choices:
          type: array
          items:
            type: object
            properties:
              text:
                type: string
                description: The edited result.
              index:
                type: integer
                format: int64
                description: The index of the choice in the list of choices.
              finish_reason:
                type: string
                enum:
                  - stop
                  - length
                description: >-
                  The reason the model stopped generating tokens. This will be
                  `stop` if the model hit a

                  natural stop point or a provided stop sequence, or `length` if
                  the maximum number of tokens

                  specified in the request was reached.
            required:
              - text
              - index
              - finish_reason
          description: >-
            description: A list of edit choices. Can be more than one if `n` is
            greater than 1.
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      required:
        - object
        - created
        - choices
        - usage
    EditN:
      type: integer
      format: int64
      minimum: 0
      maximum: 20
    LogProbs:
      type: integer
      format: int64
      minimum: 0
      maximum: 5
    MaxTokens:
      type: integer
      format: int64
      minimum: 0
    'N':
      type: integer
      format: int64
      minimum: 1
      maximum: 128
    Penalty:
      type: number
      format: double
      minimum: -2
      maximum: 2
    StopSequences:
      type: array
      items:
        type: string
      minItems: 1
      maxItems: 4
    Temperature:
      type: number
      format: double
      minimum: 0
      maximum: 2
    TokenArray:
      type: array
      items:
        type: integer
        format: int64
      minItems: 1
    TokenArrayArray:
      type: array
      items:
        $ref: '#/components/schemas/TokenArray'
      minItems: 1
    TopP:
      type: number
      format: double
      minimum: 0
      maximum: 1
    User:
      type: string
